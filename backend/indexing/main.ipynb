{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install llama-index llama-index-vector-stores-vertexaivectorsearch llama-index-llms-vertex llama-index-storage-docstore-firestore\n",
    "! pip install --upgrade google-cloud-documentai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reauthentication required.\n",
      "Please enter your password:\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://cloud.google.com/vertex-ai/docs/vector-search/quickstart#enable-apis\n",
    "! gcloud services enable compute.googleapis.com aiplatform.googleapis.com storage.googleapis.com --project ai-sandbox-company-73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "from google.cloud import aiplatform\n",
    "from vector_search_utils import get_or_create_existing_index\n",
    "from docai_parser import DocAIParser\n",
    "from google.oauth2 import service_account\n",
    "from llama_index.core import Document, Settings, StorageContext, VectorStoreIndex\n",
    "from llama_index.vector_stores.vertexaivectorsearch import VertexAIVectorStore\n",
    "from llama_index.storage.docstore.firestore import FirestoreDocumentStore\n",
    "from llama_index.embeddings.vertex import VertexTextEmbedding\n",
    "from llama_index.llms.vertex import Vertex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)  # Set the desired logging level\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from config.yaml\n",
    "def load_config():\n",
    "    config_path = os.path.join(\n",
    "        os.path.dirname(''), \"..\", \"..\", \"common\", \"config.yaml\"\n",
    "    )\n",
    "    with open(config_path) as config_file:\n",
    "        return yaml.safe_load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config()\n",
    "\n",
    "\n",
    "# Initialize parameters\n",
    "PROJECT_ID = config[\"project_id\"]\n",
    "LOCATION = config[\"location\"]\n",
    "INPUT_BUCKET_NAME = config[\"input_bucket_name\"]\n",
    "DOCSTORE_BUCKET_NAME = config[\"docstore_bucket_name\"]\n",
    "INDEX_ID = config[\"index_id\"]\n",
    "VECTOR_INDEX_NAME = config[\"vector_index_name\"]\n",
    "INDEX_ENDPOINT_NAME = config[\"index_endpoint_name\"]\n",
    "INDEXING_METHOD = config[\"indexing_method\"]\n",
    "CHUNK_SIZES = config[\"chunk_sizes\"]\n",
    "EMBEDDINGS_MODEL_NAME = config[\"embeddings_model_name\"]\n",
    "LLM_MODEL_NAME = config[\"llm_model_name\"]\n",
    "APPROXIMATE_NEIGHBORS_COUNT = config[\"approximate_neighbors_count\"]\n",
    "BUCKET_PREFIX = config[\"bucket_prefix\"]\n",
    "VECTOR_DATA_PREFIX = config[\"vector_data_prefix\"]\n",
    "CHUNK_SIZE = config.get(\"chunk_size\", 512)\n",
    "CHUNK_OVERLAP = config.get(\"chunk_overlap\", 50)\n",
    "DOCAI_LOCATION = config[\"docai_location\"]\n",
    "DOCAI_PROCESSOR_DISPLAY_NAME = config[\"document_ai_processor_display_name\"]\n",
    "DOCAI_PROCESSOR_ID = config.get(\"docai_processor_id\")\n",
    "CREATE_DOCAI_PROCESSOR = config.get(\"create_docai_processor\", False)\n",
    "FIRESTORE_DB_NAME = config.get(\"firestore_db_name\")\n",
    "FIRESTORE_NAMESPACE = config.get(\"firestore_namespace\")\n",
    "QA_INDEX_NAME = config.get(\"qa_index_name\")\n",
    "QA_ENDPOINT_NAME = config.get(\"qa_endpoint_name\")\n",
    "GCS_OUTPUT_PATH = f\"gs://{DOCSTORE_BUCKET_NAME}/{VECTOR_DATA_PREFIX}/docai_output/\"\n",
    "GOOGLE_CREDENTIAL_PATH = config.get(\"credential\")\n",
    "\n",
    "# Google Service Account credentials\n",
    "google_credential_path = os.path.join(os.path.dirname(''), \"..\", \"..\", GOOGLE_CREDENTIAL_PATH)\n",
    "google_credential = service_account.Credentials.from_service_account_file(google_credential_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing index: index_hierarchical\n",
      "Found existing endpoint: index_endpoint\n",
      "Using existing deployed index and endpoint\n"
     ]
    }
   ],
   "source": [
    "# Initialize Vertex AI and create index and endpoint\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Creating Vector Search Index\n",
    "vs_index, vs_endpoint = get_or_create_existing_index(\n",
    "    VECTOR_INDEX_NAME, INDEX_ENDPOINT_NAME, APPROXIMATE_NEIGHBORS_COUNT, google_credential\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertex AI Vector Search Vector DB and Firestore Docstore\n",
    "vector_store = VertexAIVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=LOCATION,\n",
    "    index_id=vs_index.name,  # Use .name instead of .resource_name as it contains the full path\n",
    "    endpoint_id=vs_endpoint.name,  # Use .name instead of .resource_name\n",
    "    gcs_bucket_name=DOCSTORE_BUCKET_NAME,\n",
    "    credentials_path=google_credential_path\n",
    ")\n",
    "# TODO: Add service account credentials for Firestore\n",
    "docstore = FirestoreDocumentStore.from_database(\n",
    "    project=PROJECT_ID, database=FIRESTORE_DB_NAME, namespace=FIRESTORE_NAMESPACE\n",
    ")\n",
    "\n",
    "# Setup embedding model and LLM\n",
    "embed_model = VertexTextEmbedding(\n",
    "    model_name=EMBEDDINGS_MODEL_NAME, project=PROJECT_ID, location=LOCATION, credentials=google_credential\n",
    ")\n",
    "llm = Vertex(model=LLM_MODEL_NAME, temperature=0.0)\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Initialise Document AI parser\n",
    "parser = DocAIParser(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=DOCAI_LOCATION,\n",
    "    processor_name=f\"projects/{PROJECT_ID}/locations/{DOCAI_LOCATION}/processors/{DOCAI_PROCESSOR_ID}\",  # noqa: E501\n",
    "    gcs_output_path=GCS_OUTPUT_PATH\n",
    ")\n",
    "\n",
    "# Download data from specific bucket and parse\n",
    "local_data_path = os.path.join(\"/tmp\", BUCKET_PREFIX)\n",
    "os.makedirs(local_data_path, exist_ok=True)\n",
    "blobs = create_pdf_blob_list(INPUT_BUCKET_NAME, BUCKET_PREFIX)\n",
    "logger.info(\"downloading data\")\n",
    "download_bucket_with_transfer_manager(\n",
    "    INPUT_BUCKET_NAME, prefix=BUCKET_PREFIX, destination_directory=local_data_path\n",
    ")\n",
    "\n",
    "# Parse documents using Document AI\n",
    "try:\n",
    "    parsed_docs, raw_results = parser.batch_parse(\n",
    "        blobs, chunk_size=CHUNK_SIZE, include_ancestor_headings=True\n",
    "    )\n",
    "    print(f\"Number of documents parsed by Document AI: {len(parsed_docs)}\")\n",
    "    if parsed_docs:\n",
    "        print(\n",
    "            f\"First parsed document text (first 100 chars): {parsed_docs[0].text[:100]}...\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"No documents were parsed by Document AI\")\n",
    "\n",
    "    # Print raw results for debugging\n",
    "    print(\"Raw results from Document AI:\")\n",
    "    for result in raw_results:\n",
    "        print(f\"  Source: {result.source_path}\")\n",
    "        print(f\"  Parsed: {result.parsed_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing documents: {str(e)}\")\n",
    "    parsed_docs = []\n",
    "    raw_results = []\n",
    "\n",
    "# Turn each parsed document into llama_index Document\n",
    "li_docs = [Document(text=doc.text, metadata=doc.metadata) for doc in parsed_docs]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_11_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
